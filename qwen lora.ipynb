{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:13:26.275985Z","iopub.status.busy":"2024-04-14T13:13:26.275280Z","iopub.status.idle":"2024-04-14T13:14:04.787296Z","shell.execute_reply":"2024-04-14T13:14:04.786090Z","shell.execute_reply.started":"2024-04-14T13:13:26.275945Z"},"trusted":true},"outputs":[],"source":["!pip install datasets\n","!pip install peft\n","!pip install accelerate -U"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:14:04.789737Z","iopub.status.busy":"2024-04-14T13:14:04.789442Z","iopub.status.idle":"2024-04-14T13:14:04.852505Z","shell.execute_reply":"2024-04-14T13:14:04.851618Z","shell.execute_reply.started":"2024-04-14T13:14:04.789710Z"},"trusted":true},"outputs":[],"source":["import transformers\n","from transformers import AutoTokenizer,AutoModelForCausalLM,Trainer,TrainingArguments,DataCollatorForSeq2Seq\n","import torch\n","from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","import json\n","import accelerate\n","from torch.utils.data import Dataset\n","from typing import Dict"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:14:04.854057Z","iopub.status.busy":"2024-04-14T13:14:04.853690Z","iopub.status.idle":"2024-04-14T13:14:04.861351Z","shell.execute_reply":"2024-04-14T13:14:04.860487Z","shell.execute_reply.started":"2024-04-14T13:14:04.854024Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('4.39.3', '0.28.0')"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["transformers.__version__,accelerate.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:45:06.482473Z","iopub.status.busy":"2024-04-14T13:45:06.482064Z","iopub.status.idle":"2024-04-14T13:45:06.978184Z","shell.execute_reply":"2024-04-14T13:45:06.976810Z","shell.execute_reply.started":"2024-04-14T13:45:06.482443Z"},"trusted":true},"outputs":[],"source":["train_path='/kaggle/input/ruozhiba/ruozhiba.jsonl'\n","lora_path = '/kaggle/working/lora'\n","device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","tokenizer=AutoTokenizer.from_pretrained('Qwen/Qwen1.5-1.8B-Chat',padding_side=\"right\",trust_remote_code=True,padding=True,truncation=True)\n","args = TrainingArguments(\n","    output_dir=lora_path,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=10,\n","    logging_steps=24,\n","    num_train_epochs=10,\n","    save_steps=1000,\n","    learning_rate=1e-3,\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:14:10.897750Z","iopub.status.busy":"2024-04-14T13:14:10.897446Z","iopub.status.idle":"2024-04-14T13:14:10.909662Z","shell.execute_reply":"2024-04-14T13:14:10.908686Z","shell.execute_reply.started":"2024-04-14T13:14:10.897725Z"},"trusted":true},"outputs":[],"source":["def process_func(example):\n","    input_ids, attention_mask, labels = [], [], []\n","    instruction = tokenizer(\n","'''<|im_start|>system\n","请尽最大可能回答问题。<|im_end|>\n","<|im_start|>user\n","%s<|im_end|>\n","<|im_start|>assistant\\n'''%(example['instruction']))\n","    response = tokenizer(f\"{example['output']}\")\n","    input_ids = instruction[\"input_ids\"]+response[\"input_ids\"]+[tokenizer.pad_token_id]\n","    attention_mask = instruction[\"attention_mask\"]+response[\"attention_mask\"]+[1]\n","    labels =[-100]*len(instruction[\"input_ids\"])+response[\"input_ids\"]+[tokenizer.pad_token_id]  \n","    return {\n","        \"input_ids\": torch.Tensor(input_ids).long().to(device),\n","        \"attention_mask\": torch.Tensor(attention_mask).long().to(device),\n","        \"labels\": torch.Tensor(labels).long().to(device)\n","    }\n","class MyDataset(Dataset):\n","    def __init__(self,path):\n","        super(MyDataset, self).__init__()\n","        self.data=[]\n","        with open(path,'r') as f:\n","            for i in f:\n","              now=json.loads(i)\n","              ans=process_func(now)\n","              self.data.append(ans)\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n","        return dict(\n","            input_ids=self.data[i]['input_ids'],\n","            labels=self.data[i]['labels'],\n","            attention_mask=self.data[i]['attention_mask'],\n","        )"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:14:10.911878Z","iopub.status.busy":"2024-04-14T13:14:10.911279Z","iopub.status.idle":"2024-04-14T13:14:11.322663Z","shell.execute_reply":"2024-04-14T13:14:11.321712Z","shell.execute_reply.started":"2024-04-14T13:14:10.911844Z"},"trusted":true},"outputs":[],"source":["dataset=dict(train_dataset=MyDataset(train_path),eval_dataset=None)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:14:11.324689Z","iopub.status.busy":"2024-04-14T13:14:11.323912Z","iopub.status.idle":"2024-04-14T13:14:11.330751Z","shell.execute_reply":"2024-04-14T13:14:11.329668Z","shell.execute_reply.started":"2024-04-14T13:14:11.324651Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torch.Tensor'>\n"]}],"source":["print(type(dataset['train_dataset'][100]['input_ids']))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:14:11.332171Z","iopub.status.busy":"2024-04-14T13:14:11.331858Z","iopub.status.idle":"2024-04-14T13:15:15.031741Z","shell.execute_reply":"2024-04-14T13:15:15.030906Z","shell.execute_reply.started":"2024-04-14T13:14:11.332147Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27f1e45e216d48778e3abcb34e432a9a","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"466b31bf7b8347bfa10fc517a4fe0cbf","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1fb94f695aa4840ac3ce387ba0c544d","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model=AutoModelForCausalLM.from_pretrained('Qwen/Qwen1.5-1.8B-Chat',torch_dtype=torch.float32).to(device)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:15:15.033172Z","iopub.status.busy":"2024-04-14T13:15:15.032896Z","iopub.status.idle":"2024-04-14T13:15:15.175688Z","shell.execute_reply":"2024-04-14T13:15:15.174873Z","shell.execute_reply.started":"2024-04-14T13:15:15.033149Z"},"trusted":true},"outputs":[],"source":["peft_config=LoraConfig(\n","         r=16,\n","         lora_alpha=16,\n","         target_modules=[\"q_proj\",\"k_proj\"],\n","         lora_dropout=0.05,\n","         bias=\"none\",\n","         task_type=\"CAUSAL_LM\")\n","lora_model=get_peft_model(model,peft_config)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:15:15.179705Z","iopub.status.busy":"2024-04-14T13:15:15.179019Z","iopub.status.idle":"2024-04-14T13:15:15.187842Z","shell.execute_reply":"2024-04-14T13:15:15.186861Z","shell.execute_reply.started":"2024-04-14T13:15:15.179676Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["all:1839974400||train3145728||train%0.17096585691627014\n"]}],"source":["def print_trainable_param(model):\n","  trainable=0\n","  all=0\n","  for _,param in model.named_parameters():\n","    all+=param.numel()\n","    if param.requires_grad:\n","      trainable+=param.numel()\n","  print(f\"all:{all}||train{trainable}||train%{100*trainable/all}\")\n","print_trainable_param(model)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:46:05.877874Z","iopub.status.busy":"2024-04-14T13:46:05.877314Z","iopub.status.idle":"2024-04-14T13:46:05.898067Z","shell.execute_reply":"2024-04-14T13:46:05.897079Z","shell.execute_reply.started":"2024-04-14T13:46:05.877843Z"},"trusted":true},"outputs":[],"source":["trainer = Trainer(\n","    model=lora_model,\n","    args=args,\n","    tokenizer=tokenizer,\n","    **dataset\n",")"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:03:56.820108Z","iopub.status.busy":"2024-04-14T14:03:56.819699Z","iopub.status.idle":"2024-04-14T14:11:13.737088Z","shell.execute_reply":"2024-04-14T14:11:13.735666Z","shell.execute_reply.started":"2024-04-14T14:03:56.820078Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='171' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [171/240 07:13 < 02:56, 0.39 it/s, Epoch 7.08/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>24</td>\n","      <td>0.119600</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.117800</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.118400</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.088100</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.059000</td>\n","    </tr>\n","    <tr>\n","      <td>144</td>\n","      <td>0.039800</td>\n","    </tr>\n","    <tr>\n","      <td>168</td>\n","      <td>0.028600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2001\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;124;03m    Scales the gradients in accordance to the `GradientAccumulationPlugin` and calls the correct `backward()` based\u001b[39;00m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;124;03m    on the configuration.\u001b[39;00m\n\u001b[1;32m   1989\u001b[0m \n\u001b[1;32m   1990\u001b[0m \u001b[38;5;124;03m    Should be used in lieu of `loss.backward()`.\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \n\u001b[1;32m   1992\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \n\u001b[1;32m   1994\u001b[0m \u001b[38;5;124;03m    ```python\u001b[39;00m\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;124;03m    >>> from accelerate import Accelerator\u001b[39;00m\n\u001b[1;32m   1996\u001b[0m \n\u001b[1;32m   1997\u001b[0m \u001b[38;5;124;03m    >>> accelerator = Accelerator(gradient_accumulation_steps=2)\u001b[39;00m\n\u001b[1;32m   1998\u001b[0m \u001b[38;5;124;03m    >>> outputs = model(inputs)\u001b[39;00m\n\u001b[1;32m   1999\u001b[0m \u001b[38;5;124;03m    >>> loss = loss_fn(outputs, labels)\u001b[39;00m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;124;03m    >>> accelerator.backward(loss)\u001b[39;00m\n\u001b[0;32m-> 2001\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   2004\u001b[0m         \u001b[38;5;66;03m# deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:11:24.688087Z","iopub.status.busy":"2024-04-14T14:11:24.687691Z","iopub.status.idle":"2024-04-14T14:11:29.103524Z","shell.execute_reply":"2024-04-14T14:11:29.102499Z","shell.execute_reply.started":"2024-04-14T14:11:24.688058Z"},"trusted":true},"outputs":[],"source":["tokenizer.padding_side='left'\n","model_inputs = tokenizer(['''<|im_start|>system\n","请尽最大可能回答问题。<|im_end|>\n","<|im_start|>user\n","老师说提一分干掉千人，那我干掉千人是不是就相当于提了一分？<|im_end|>\n","<|im_start|>assistant\\n'''], return_tensors=\"pt\",padding=True).to(device)\n","generated_ids=lora_model.generate(model_inputs.input_ids,max_new_tokens=512)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T13:26:26.432582Z","iopub.status.busy":"2024-04-14T13:26:26.432131Z","iopub.status.idle":"2024-04-14T13:26:26.440277Z","shell.execute_reply":"2024-04-14T13:26:26.439396Z","shell.execute_reply.started":"2024-04-14T13:26:26.432546Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['不一定。虽然“提一分干掉千人”意味着老师要求学生在短时间内取得一定的成绩或者进步，但是提的分数通常是指学生的最终得分或考试分数，而不仅仅是他们在某一方面的表现。\\n\\n例如，如果一个学生在数学考试中得了80分，这并不等于他在所有科目的考试中都得到了相同的平均分。他可能会在某些科目中表现得更好（比如他的英语考试成绩可能是全班最高的），而在其他科目中的表现可能较差。因此，“提一分干掉千人”实际上代表了他在某个特定领域的总分或平均分达到了很高的水平，而不是在整个考试成绩序列中取得了相同的成绩。\\n\\n另外，即使一个人在某一科目的成绩比其他人高，但在其他科目上的表现可能仍然不如人意。例如，在数学课程中，一个学生在一次重要考试中的优秀成绩并不能保证他在其他学科中的高排名。相反，他需要在其他科目中保持优异的表现，同时也要通过各种方式提高自己的学习能力和适应性。\\n\\n综上所述，即使一个人在“提一分干掉千人”的情况下取得了较高的分数，也不等于他们获得了与“千人”相同的整体成就，因为他们的最终成绩和表现是基于多种因素的综合评价。']\n"]}],"source":["generated_ids = [\n","    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids,generated_ids)\n","]\n","response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","print(response)"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T14:11:29.107239Z","iopub.status.busy":"2024-04-14T14:11:29.106941Z","iopub.status.idle":"2024-04-14T14:11:29.115731Z","shell.execute_reply":"2024-04-14T14:11:29.114292Z","shell.execute_reply.started":"2024-04-14T14:11:29.107213Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['这是我们上学时经常能听到的一句话，意思是在高考等重要考试中，每提高一分，就能超过很多人。这是因为考试成绩通常呈现出正态分布，所以分数越高，超越的人数就越多。这个说法并不是字面意义上的“干掉”，而是指在竞争激烈的考试中超过其他考生。\\n\\n因此，如果你在考试中提高了一分，确实意味着在成绩排名上超过了很多人，但这并不是实际上的“干掉”或伤害别人，而只是在分数上超越了他们。这句话强调的是在激烈的学术竞争中，即使是很小的分数提升也可能意味着在排名上的大幅跃升。<|endoftext|>']\n"]}],"source":["generated_ids = [\n","    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids,generated_ids)\n","]\n","response = tokenizer.batch_decode(generated_ids)\n","print(response)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4794543,"sourceId":8116124,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
